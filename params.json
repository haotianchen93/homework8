{
  "name": "Homework 8 - Topic Modeling",
  "tagline": "Amazon Fine Food Reviews",
  "body": "#Haotian Chen\r\n\r\n##Summary\r\n\r\nIn this project, I run an LDA topic modeling analysis for Amazon Fine Food Reviews (available from: https://www.kaggle.com/snap/amazon-fine-food-reviews). I used the R package lda and I visualize the output using LDAvis.\r\n\r\nThe R Source Code is Available at: https://github.com/haotianchen93/homework8/blob/master/Homework%208.R\r\n\r\nThe LDA topic models analysis for top 10 relavant topics is Available at: https://haotianchen93.github.io/homework8/vis\r\n\r\n##Result Analysis\r\n\r\nTopic 5 & 7 have large overlaps, because these customer reviews focused on information of taste and flavor. Topic 2 & 6 have some overlaps, mostly focused on various dimensions of differnet kinds of food. Topic 1 & 3 also overlap, and these two focused on the service Amazon provided, such as box, time, and price. Topic 4/8/9/10 are isolated, primarily because these four topics focused on different food: sugar, pet food, tea and coffee respectively. So we can see that these reviews are very comprehensive and varied.\r\n\r\n![](https://github.com/haotianchen93/homework8/blob/master/Screen%20Shot%202016-11-15%20at%204.54.01%20PM.png?raw=true)\r\n\r\n##The data\r\n\r\nFirst, I manually download the review .csv file to my mac and load the .csv file in R studio.\r\n\r\n```r\r\n#Topic Modeling: Amazon Fine Food Reviews\r\n\r\nlibrary(dplyr)\r\nrequire(magrittr)\r\nlibrary(tm)\r\nlibrary(ggplot2)\r\nlibrary(stringr)\r\nlibrary(NLP)\r\nlibrary(openNLP)\r\n\r\n#load csv file\r\nprecorpus<- read.csv(\"/Users/chenhaotian/Desktop/amazon reviews.csv\", header=TRUE, stringsAsFactors=FALSE)\r\n\r\n#passing Full Text to variable review\r\nreview <- precorpus$Text\r\n```\r\n\r\n##Pre-processing\r\n\r\nBefore fitting a topic model, I need to tokenize the text and remove all the punctuations and spaces. In particular, we use the English stop words from the \"SMART\" and some customized stop words.\r\n\r\n```r\r\n#Cleaning corpus\r\nstop_words <- stopwords(\"SMART\")\r\n## additional junk words showing up in the data\r\nstop_words <- c(stop_words, \"said\", \"the\", \"also\", \"say\", \"just\", \"like\",\"for\", \r\n                \"us\", \"can\", \"may\", \"now\", \"year\", \"according\", \"mr\", \"br\", \"www\", \"http\")\r\nstop_words <- tolower(stop_words)\r\n\r\n\r\nreview <- gsub(\"'\", \"\", review) # remove apostrophes\r\nreview <- gsub(\"[[:punct:]]\", \" \", review)  # replace punctuation with space\r\nreview <- gsub(\"[[:cntrl:]]\", \" \", review)  # replace control characters with space\r\nreview <- gsub(\"^[[:space:]]+\", \"\", review) # remove whitespace at beginning of documents\r\nreview <- gsub(\"[[:space:]]+$\", \"\", review) # remove whitespace at end of documents\r\nreview <- gsub(\"[^a-zA-Z -]\", \" \", review) # allows only letters\r\nreview <- tolower(review)  # force to lowercase\r\n\r\n## get rid of blank docs\r\nreview <- review[review != \"\"]\r\n\r\n# tokenize on space and output as a list:\r\ndoc.list <- strsplit(review, \"[[:space:]]+\")\r\n\r\n# compute the table of terms:\r\nterm.table <- table(unlist(doc.list))\r\nterm.table <- sort(term.table, decreasing = TRUE)\r\n\r\n\r\n# remove terms that are stop words or occur fewer than 5 times:\r\ndel <- names(term.table) %in% stop_words | term.table < 5\r\nterm.table <- term.table[!del]\r\nterm.table <- term.table[names(term.table) != \"\"]\r\nvocab <- names(term.table)\r\n\r\n# now put the documents into the format required by the lda package:\r\nget.terms <- function(x) {\r\n  index <- match(x, vocab)\r\n  index <- index[!is.na(index)]\r\n  rbind(as.integer(index - 1), as.integer(rep(1, length(index))))\r\n}\r\ndocuments <- lapply(doc.list, get.terms)\r\n```\r\n\r\n##Using R package 'lda' for model fitting\r\n\r\nThe object document is a large list where each element represents one document. After creating this list, we compute a few statistics about the corpus, such as length and vocabulary counts:\r\n\r\n```r\r\n# Compute some statistics related to the data set:\r\nD <- length(documents)  # number of documents (1)\r\nW <- length(vocab)  # number of terms in the vocab (1741)\r\ndoc.length <- sapply(documents, function(x) sum(x[2, ]))  # number of tokens per document [312, 288, 170, 436, 291, ...]\r\nN <- sum(doc.length)  # total number of tokens in the data (56196)\r\nterm.frequency <- as.integer(term.table) \r\n```\r\n\r\nNext, we set up a topic model with 10 topics, relatively diffuse priors for the topic-term distributions ($\\eta$ = 0.02) and document-topic distributions ($\\alpha$ = 0.02), and we set the collapsed Gibbs sampler to run for 3,000 iterations (slightly conservative to ensure convergence). A visual inspection of fit$log.likelihood shows that the MCMC algorithm has converged after 3,000 iterations. This block of code takes about 50 seconds to run on a Mac using a 2.4GHz i7 processor and 8GB RAM.\r\n\r\n```r\r\n# MCMC and model tuning parameters:\r\nK <- 10\r\nG <- 3000\r\nalpha <- 0.02\r\neta <- 0.02\r\n\r\n# Fit the model:\r\nlibrary(lda)\r\nset.seed(357)\r\nt1 <- Sys.time()\r\nfit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab, \r\n                                   num.iterations = G, alpha = alpha, \r\n                                   eta = eta, initial = NULL, burnin = 0,\r\n                                   compute.log.likelihood = TRUE)\r\nt2 <- Sys.time()\r\n## display runtime\r\nt2 - t1  \r\n```\r\n\r\n##Visualizing the fitted model with LDAvis\r\n\r\nTo visualize the result using LDAvis, we will need estimates of the document-topic distributions, which we denote by the $D \\times K$ matrix $\\theta$, and the set of topic-term distributions, which we denote by the $K \\times W$ matrix $\\phi$. We estimate the \"smoothed\" versions of these distributions (\"smoothed\" means that we've incorporated the effects of the priors into the estimates) by cross-tabulating the latent topic assignments from the last iteration of the collapsed Gibbs sampler with the documents and the terms, respectively, and then adding pseudocounts according to the priors. A better estimator might average over multiple iterations of the Gibbs sampler (after convergence, assuming that the MCMC is sampling within a local mode and there is no label switching occurring), but we won't worry about that for now.\r\n\r\nWe've already computed the number of tokens per document and the frequency of the terms across the entire corpus. We save these, along with $\\phi$, $\\theta$, and vocab, in a list as the data object reviews.LDA.\r\n\r\n```r\r\ntheta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))\r\nphi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))\r\n\r\nreview_for_LDA <- list(phi = phi,\r\n                     theta = theta,\r\n                     doc.length = doc.length,\r\n                     vocab = vocab,\r\n                     term.frequency = term.frequency)\r\n```\r\n\r\nNow we're ready to call the createJSON() function in LDAvis. This function will return a character string representing a JSON object used to populate the visualization. The createJSON() function computes topic frequencies, inter-topic distances, and projects topics onto a two-dimensional plane to represent their similarity to each other. It also loops through a grid of values of a tuning parameter, $0 \\leq \\lambda \\leq 1$, that controls how the terms are ranked for each topic, where terms are listed in decreasing of relevance, where the relevance of term $w$ to topic $t$ is defined as $\\lambda \\times p(w \\mid t) + (1 - \\lambda) \\times p(w \\mid t)/p(w)$. Values of $\\lambda$ near 1 give high relevance rankings to frequent terms within a given topic, whereas values of $\\lambda$ near zero give high relevance rankings to exclusive terms within a topic. The set of all terms which are ranked among the top-R most relevant terms for each topic are pre-computed by the createJSON() function and sent to the browser to be interactively visualized using D3 as part of the JSON object.\r\n\r\n```r\r\nlibrary(LDAvis)\r\nlibrary(servr)\r\n\r\n# create the JSON object to feed the visualization:\r\njson <- createJSON(phi = review_for_LDA$phi, \r\n                   theta = review_for_LDA$theta, \r\n                   doc.length = review_for_LDA$doc.length, \r\n                   vocab = review_for_LDA$vocab, \r\n                   term.frequency = review_for_LDA$term.frequency)\r\n\r\nserVis(json, out.dir = 'vis', open.browser = TRUE)\r\n```\r\n\r\nThe serVis() function can take json and serve the result in a variety of ways. Here we'll write json to a file within the 'vis' directory (along with other HTML and JavaScript required to render the page). You can see the result at: https://haotianchen93.github.io/homework8/vis",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}